{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0+cu110 available.\n",
      "TensorFlow version 2.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import * \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "import time \n",
    "import datetime \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2834, 6), (7, 4), (7, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv') \n",
    "test = pd.read_csv('./data/test.csv') \n",
    "submission = pd.read_csv('./data/sample_submission.csv') \n",
    "\n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train['excerpt'].values \n",
    "train_targets = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\") \n",
    "\n",
    "def roberta_tokenizer(sent, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    if len(input_id) > 512: \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_maks[:129] + attention_mask[-383:] \n",
    "        print(\"Long Text!! Using Head+Tail Truncation\") \n",
    "    elif len(input_id) <= 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512-len(attention_mask)) \n",
    "        \n",
    "    return input_id, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10 \n",
    "VALID_SPLIT = 0.1 \n",
    "MAX_LEN = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s): \n",
    "    # find alphabets\n",
    "    cleaned = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "    # convert to lower case\n",
    "    cleaned = s.lower()\n",
    "    return cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:02<00:00, 1194.32it/s]\n"
     ]
    }
   ],
   "source": [
    "N = train_texts.shape[0] \n",
    "\n",
    "input_ids = np.zeros((N,MAX_LEN),dtype=int) \n",
    "attention_masks = np.zeros((N, MAX_LEN),dtype=int) \n",
    "targets = np.zeros((N), dtype=np.double) \n",
    "\n",
    "for i in tqdm(range(N), position = 0, leave=True): \n",
    "    try: \n",
    "        cur_str = train_texts[i] \n",
    "        cur_target = train_targets[i] \n",
    "        input_id, attention_mask = roberta_tokenizer(cur_str, MAX_LEN=MAX_LEN) \n",
    "        input_ids[i,] = input_id \n",
    "        attention_masks[i,] = attention_mask \n",
    "        targets[i,] = cur_target \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(cur_str) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2550, 512]), torch.Size([2550, 512]), torch.Size([2550]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int) \n",
    "targets = torch.tensor(targets, dtype=torch.float32) \n",
    "\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_ids, targets, random_state = 42, test_size=VALID_SPLIT)\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(attention_masks, targets, random_state = 42, test_size=VALID_SPLIT) \n",
    "\n",
    "train_inputs.shape, train_attention_mask.shape, train_targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([284, 512]), torch.Size([284, 512]), torch.Size([284]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_attention_mask.shape, val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_attention_mask, train_targets) \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE) \n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_attention_mask, val_targets)\n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1) \n",
    "model.cuda() \n",
    "print() # avoid printing model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 1.3051962792873382\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.9491456530988216\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.780700970441103\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.6959482306614518\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.6563096590340137\n",
      "   Batch   120 of   160. Elapsed: 0:01:03.\n",
      "   Current average loss = 0.6181190304458142\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.5832625820168427\n",
      "Average training loss = 0.5697020191699267\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.5131287425756454\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.24603207483887674\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.2673688707873225\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.31987807316084704\n",
      "   Batch    80 of   160. Elapsed: 0:00:43.\n",
      "   Current average loss = 0.3167695990763605\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.3065250650793314\n",
      "   Batch   120 of   160. Elapsed: 0:01:04.\n",
      "   Current average loss = 0.2935465273757776\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.28659508222980157\n",
      "Average training loss = 0.2836883363779634\n",
      "Training epoch took = 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.38520796928140855\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.18723115138709545\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.17817886294797064\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.1776371314500769\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.1719154360704124\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.17739077545702459\n",
      "   Batch   120 of   160. Elapsed: 0:01:03.\n",
      "   Current average loss = 0.1812751819069187\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.18722687289118767\n",
      "Average training loss = 0.18590333440806717\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.31276736822393203\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.12293944284319877\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.13217013981193304\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.13133675965170066\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.13238067873753606\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.13244426175951957\n",
      "   Batch   120 of   160. Elapsed: 0:01:04.\n",
      "   Current average loss = 0.13188672782853245\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.12870132047683\n",
      "Average training loss = 0.12729671460110695\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.32025690293974346\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.12069657947868109\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.10866621574386955\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.10068087795128425\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.09778520548716188\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.09529991336166858\n",
      "   Batch   120 of   160. Elapsed: 0:01:03.\n",
      "   Current average loss = 0.09768992181246479\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.09816021744960121\n",
      "Average training loss = 0.09781699591549113\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.47567561517159146\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.07095214072614908\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.06972803100943566\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.07194864184906086\n",
      "   Batch    80 of   160. Elapsed: 0:00:43.\n",
      "   Current average loss = 0.07128960427362471\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.06970735596492887\n",
      "   Batch   120 of   160. Elapsed: 0:01:04.\n",
      "   Current average loss = 0.06964318167107801\n",
      "   Batch   140 of   160. Elapsed: 0:01:15.\n",
      "   Current average loss = 0.06871125376118081\n",
      "Average training loss = 0.06794725570362062\n",
      "Training epoch took = 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3048551720049646\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.05838084341958165\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.057712150691077116\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.0545478160182635\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.05324953158851713\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.05256310351192951\n",
      "   Batch   120 of   160. Elapsed: 0:01:04.\n",
      "   Current average loss = 0.05150108456922074\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.05143390393682889\n",
      "Average training loss = 0.051271603431086984\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3856403488251898\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.04373646453022957\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.04222117683384567\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.04086049312415222\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.03977336343377828\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.0398248546756804\n",
      "   Batch   120 of   160. Elapsed: 0:01:03.\n",
      "   Current average loss = 0.03980322551603119\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.03908063414107476\n",
      "Average training loss = 0.0391536830808036\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3954315417342716\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.02921073455363512\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.03302373280748725\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.03205568557605147\n",
      "   Batch    80 of   160. Elapsed: 0:00:42.\n",
      "   Current average loss = 0.03233376645948738\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.030975926890969275\n",
      "   Batch   120 of   160. Elapsed: 0:01:03.\n",
      "   Current average loss = 0.032473612933730085\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.03194200975288238\n",
      "Average training loss = 0.03222512823995203\n",
      "Training epoch took = 0:01:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.44951145682069993\n",
      "Validation took: 0:00:03\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   160. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.027621201425790786\n",
      "   Batch    40 of   160. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.028871899470686913\n",
      "   Batch    60 of   160. Elapsed: 0:00:32.\n",
      "   Current average loss = 0.029218211692447465\n",
      "   Batch    80 of   160. Elapsed: 0:00:43.\n",
      "   Current average loss = 0.02940706758527085\n",
      "   Batch   100 of   160. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.0291951294708997\n",
      "   Batch   120 of   160. Elapsed: 0:01:04.\n",
      "   Current average loss = 0.029230267189753554\n",
      "   Batch   140 of   160. Elapsed: 0:01:14.\n",
      "   Current average loss = 0.02915139362615134\n",
      "Average training loss = 0.028843192977365106\n",
      "Training epoch took = 0:01:25\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss = 0.407351811726888\n",
      "Validation took: 0:00:03\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps) \n",
    "\n",
    "\n",
    "def format_time(elapsed):  \n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.zero_grad() \n",
    "\n",
    "for epoch_i in range(0,epochs): \n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs)) \n",
    "    print(\"Training ...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time()-t0)\n",
    "            print('   Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) \n",
    "            print('   Current average loss = {}'.format(total_loss / step)) \n",
    "            \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids = None, \n",
    "                        attention_mask = b_input_masks,\n",
    "                        labels = b_target) \n",
    "        \n",
    "        # using MSE loss \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    \n",
    "    print(\"Average training loss = {}\".format(avg_train_loss)) \n",
    "    print(\"Training epoch took = {}\".format(format_time(time.time() - t0)))\n",
    " \n",
    "    ##### validation ##### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\") \n",
    "    \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    nb_eval_steps, nb_eval_examples = 0,0 \n",
    "    \n",
    "    for batch in validation_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids = None, \n",
    "                            attention_mask = b_input_masks, \n",
    "                            labels = b_target) \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        eval_loss += loss.item() \n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)\n",
    "    print(\"Average validation loss = {}\".format(avg_val_loss)) \n",
    "    print(\"Validation took: {:}\".format(format_time(time.time()-t0))) \n",
    "    torch.save(model.state_dict(), \"RoBERTa_baseline_\" + str(epoch_i+1)) \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load best model \n",
    "test_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1) \n",
    "checkpoint = torch.load('RoBERTa_baseline_6') \n",
    "test_model.load_state_dict(checkpoint)\n",
    "test_model.cuda() \n",
    "\n",
    "test_model.eval() # convert to evaluation mode. \n",
    "print() # avoid printing model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 55.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_texts = test['excerpt'].values \n",
    "\n",
    "predictions = [] \n",
    "\n",
    "for text in tqdm(test_texts, position=0,leave=True): \n",
    "    input_id, attention_mask = roberta_tokenizer(text, MAX_LEN=MAX_LEN) \n",
    "    input_id = torch.tensor(input_id, dtype=int) \n",
    "    attention_mask = torch.tensor(attention_mask, dtype=int) \n",
    "    \n",
    "    input_id = torch.reshape(input_id, (-1,MAX_LEN)) \n",
    "    attention_mask = torch.reshape(attention_mask, (-1,MAX_LEN)) \n",
    "    \n",
    "    input_id = input_id.to(device) \n",
    "    attention_mask = attention_mask.to(device) \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "            outputs = test_model(input_id, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=attention_mask) \n",
    "    \n",
    "    yhat = outputs[0].item() \n",
    "    predictions.append(yhat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>0.372574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.225584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.119132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.694268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.715674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-0.712704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.168142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661  0.372574\n",
       "1  f0953f0a5 -0.225584\n",
       "2  0df072751 -0.119132\n",
       "3  04caf4e0c -2.694268\n",
       "4  0e63f8bea -1.715674\n",
       "5  12537fe78 -0.712704\n",
       "6  965e592c0  0.168142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[:,1] = predictions \n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
