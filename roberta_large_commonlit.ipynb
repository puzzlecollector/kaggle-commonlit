{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0+cu110 available.\n",
      "TensorFlow version 2.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import * \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "import time \n",
    "import datetime \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2834, 6), (7, 4), (7, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv') \n",
    "test = pd.read_csv('./data/test.csv') \n",
    "submission = pd.read_csv('./data/sample_submission.csv') \n",
    "\n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train['excerpt'].values \n",
    "train_targets = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\") \n",
    "\n",
    "def roberta_tokenizer(sent, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    if len(input_id) > 512: \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_maks[:129] + attention_mask[-383:] \n",
    "        print(\"Long Text!! Using Head+Tail Truncation\") \n",
    "    elif len(input_id) <= 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512-len(attention_mask)) \n",
    "        \n",
    "    return input_id, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10 \n",
    "VALID_SPLIT = 0.1 \n",
    "MAX_LEN = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s): \n",
    "    # find alphabets\n",
    "    cleaned = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "    # convert to lower case\n",
    "    cleaned = s.lower()\n",
    "    return cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:02<00:00, 1126.69it/s]\n"
     ]
    }
   ],
   "source": [
    "N = train_texts.shape[0] \n",
    "\n",
    "input_ids = np.zeros((N,MAX_LEN),dtype=int) \n",
    "attention_masks = np.zeros((N, MAX_LEN),dtype=int) \n",
    "targets = np.zeros((N), dtype=np.double) \n",
    "\n",
    "for i in tqdm(range(N), position = 0, leave=True): \n",
    "    try: \n",
    "        cur_str = train_texts[i] \n",
    "        cur_target = train_targets[i] \n",
    "        input_id, attention_mask = roberta_tokenizer(cur_str, MAX_LEN=MAX_LEN) \n",
    "        input_ids[i,] = input_id \n",
    "        attention_masks[i,] = attention_mask \n",
    "        targets[i,] = cur_target \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(cur_str) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2550, 512]), torch.Size([2550, 512]), torch.Size([2550]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int) \n",
    "targets = torch.tensor(targets, dtype=torch.float32) \n",
    "\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_ids, targets, random_state = 42, test_size=VALID_SPLIT)\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(attention_masks, targets, random_state = 42, test_size=VALID_SPLIT) \n",
    "\n",
    "train_inputs.shape, train_attention_mask.shape, train_targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([284, 512]), torch.Size([284, 512]), torch.Size([284]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_attention_mask.shape, val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_attention_mask, train_targets) \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE) \n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_attention_mask, val_targets)\n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=1) \n",
    "model.cuda() \n",
    "print() # avoid printing model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):  \n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 1.6143383882939815\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 1.2977006256580352\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 1.2457288133601347\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 1.2682549266144634\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 1.179296936392784\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 1.1275114925578236\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 1.0678264158777893\n",
      "   Batch   160 of   638. Elapsed: 0:01:21.\n",
      "   Current average loss = 1.0097389032715\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.966230737304108\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.9048311510961503\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.906846819403158\n",
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.8748076378290232\n",
      "   Batch   260 of   638. Elapsed: 0:02:12.\n",
      "   Current average loss = 0.8404625516886322\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.8097831385808864\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.7818646800083419\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.7672079071810003\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.745426041517845\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.7323593852368908\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.719331845204885\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.6970534299174324\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.693195901159197\n",
      "   Batch   440 of   638. Elapsed: 0:03:45.\n",
      "   Current average loss = 0.6865532720249824\n",
      "   Batch   460 of   638. Elapsed: 0:03:55.\n",
      "   Current average loss = 0.6756778118848477\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.6673431793654648\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.6553688349910081\n",
      "   Batch   520 of   638. Elapsed: 0:04:26.\n",
      "   Current average loss = 0.6445463191037282\n",
      "   Batch   540 of   638. Elapsed: 0:04:36.\n",
      "   Current average loss = 0.6296331409123485\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.6199719875152888\n",
      "   Batch   580 of   638. Elapsed: 0:04:56.\n",
      "   Current average loss = 0.6156108717883713\n",
      "   Batch   600 of   638. Elapsed: 0:05:07.\n",
      "   Current average loss = 0.6173830551809321\n",
      "   Batch   620 of   638. Elapsed: 0:05:17.\n",
      "   Current average loss = 0.61206119981324\n",
      "Average training loss = 0.6076461547506874\n",
      "Training epoch took = 0:05:26\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.5515608262847846\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.33357650665566324\n",
      "   Batch    40 of   638. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.3471144441049546\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.32601348714282113\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.31612578346394005\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.3328134190663695\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.32368282387033104\n",
      "   Batch   140 of   638. Elapsed: 0:01:12.\n",
      "   Current average loss = 0.3259496988196458\n",
      "   Batch   160 of   638. Elapsed: 0:01:22.\n",
      "   Current average loss = 0.328842903743498\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.33967291646533543\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.3345536570996046\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.32832630232653837\n",
      "   Batch   240 of   638. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.32019122621083324\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.3209293359407009\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.3246889719689664\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.3266645280287291\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.3244690456907847\n",
      "   Batch   340 of   638. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.3163088864277062\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.31422967046835565\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.3106194230200919\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.3148485693440307\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.3153405669289419\n",
      "   Batch   440 of   638. Elapsed: 0:03:45.\n",
      "   Current average loss = 0.32348220974143427\n",
      "   Batch   460 of   638. Elapsed: 0:03:55.\n",
      "   Current average loss = 0.3221146868875898\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.3190780543809524\n",
      "   Batch   500 of   638. Elapsed: 0:04:16.\n",
      "   Current average loss = 0.3181004037940875\n",
      "   Batch   520 of   638. Elapsed: 0:04:26.\n",
      "   Current average loss = 0.31480172480761004\n",
      "   Batch   540 of   638. Elapsed: 0:04:36.\n",
      "   Current average loss = 0.31138620726436517\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.3083186787295355\n",
      "   Batch   580 of   638. Elapsed: 0:04:57.\n",
      "   Current average loss = 0.3080565291882396\n",
      "   Batch   600 of   638. Elapsed: 0:05:07.\n",
      "   Current average loss = 0.3067314857748958\n",
      "   Batch   620 of   638. Elapsed: 0:05:17.\n",
      "   Current average loss = 0.304982831089517\n",
      "Average training loss = 0.3050067606725114\n",
      "Training epoch took = 0:05:26\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.48398008013904936\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.29210810689255595\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.3597288269083947\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.35628950195387005\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.30876043331809344\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.2985714561492205\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.28052148870192467\n",
      "   Batch   140 of   638. Elapsed: 0:01:12.\n",
      "   Current average loss = 0.2695632774781968\n",
      "   Batch   160 of   638. Elapsed: 0:01:22.\n",
      "   Current average loss = 0.2624729509814642\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.2534606567583978\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.2536148515343666\n",
      "   Batch   220 of   638. Elapsed: 0:01:53.\n",
      "   Current average loss = 0.2517919703640721\n",
      "   Batch   240 of   638. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.25104200358813006\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.24655782113997982\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.2434907488924052\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.2382588217469553\n",
      "   Batch   320 of   638. Elapsed: 0:02:44.\n",
      "   Current average loss = 0.24041863449965603\n",
      "   Batch   340 of   638. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.23988776446374901\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.23510283234322238\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.23074835981487443\n",
      "   Batch   400 of   638. Elapsed: 0:03:25.\n",
      "   Current average loss = 0.22755621776450424\n",
      "   Batch   420 of   638. Elapsed: 0:03:35.\n",
      "   Current average loss = 0.22553873468811314\n",
      "   Batch   440 of   638. Elapsed: 0:03:45.\n",
      "   Current average loss = 0.223948383750394\n",
      "   Batch   460 of   638. Elapsed: 0:03:56.\n",
      "   Current average loss = 0.2200358038444234\n",
      "   Batch   480 of   638. Elapsed: 0:04:06.\n",
      "   Current average loss = 0.21703211743539821\n",
      "   Batch   500 of   638. Elapsed: 0:04:16.\n",
      "   Current average loss = 0.2175101800635457\n",
      "   Batch   520 of   638. Elapsed: 0:04:26.\n",
      "   Current average loss = 0.2156193415992535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Batch   540 of   638. Elapsed: 0:04:37.\n",
      "   Current average loss = 0.21553837612823204\n",
      "   Batch   560 of   638. Elapsed: 0:04:47.\n",
      "   Current average loss = 0.21583792032407864\n",
      "   Batch   580 of   638. Elapsed: 0:04:57.\n",
      "   Current average loss = 0.21594577036027252\n",
      "   Batch   600 of   638. Elapsed: 0:05:08.\n",
      "   Current average loss = 0.21210878489228585\n",
      "   Batch   620 of   638. Elapsed: 0:05:18.\n",
      "   Current average loss = 0.21045429894039708\n",
      "Average training loss = 0.20952675357274128\n",
      "Training epoch took = 0:05:27\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.24676763305676655\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:11.\n",
      "   Current average loss = 0.13915174249559642\n",
      "   Batch    40 of   638. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.15996879409067333\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.16275491500273348\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.15937889369670302\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.16017470737919212\n",
      "   Batch   120 of   638. Elapsed: 0:01:02.\n",
      "   Current average loss = 0.1498902536618213\n",
      "   Batch   140 of   638. Elapsed: 0:01:12.\n",
      "   Current average loss = 0.14417848887720278\n",
      "   Batch   160 of   638. Elapsed: 0:01:22.\n",
      "   Current average loss = 0.1400726369698532\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.13944449845908416\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.1409955808147788\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.14211616872979158\n",
      "   Batch   240 of   638. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.14207091416465117\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.14206428697977502\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.14171231203779047\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.14263604933706422\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.14199385480897037\n",
      "   Batch   340 of   638. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.14256496412734337\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.1434179802916737\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.1433270799951922\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.14197725791251287\n",
      "   Batch   420 of   638. Elapsed: 0:03:35.\n",
      "   Current average loss = 0.14025359064828427\n",
      "   Batch   440 of   638. Elapsed: 0:03:45.\n",
      "   Current average loss = 0.1392496940912679\n",
      "   Batch   460 of   638. Elapsed: 0:03:55.\n",
      "   Current average loss = 0.13870061244208204\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.13785221720463597\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.13753924799046946\n",
      "   Batch   520 of   638. Elapsed: 0:04:26.\n",
      "   Current average loss = 0.1379265401806781\n",
      "   Batch   540 of   638. Elapsed: 0:04:36.\n",
      "   Current average loss = 0.1385244279141598\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.13717943483570708\n",
      "   Batch   580 of   638. Elapsed: 0:04:56.\n",
      "   Current average loss = 0.13837867601491624\n",
      "   Batch   600 of   638. Elapsed: 0:05:07.\n",
      "   Current average loss = 0.137966524658162\n",
      "   Batch   620 of   638. Elapsed: 0:05:17.\n",
      "   Current average loss = 0.13774330750954591\n",
      "Average training loss = 0.1373418885724565\n",
      "Training epoch took = 0:05:26\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3083792959772785\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.09578184294514358\n",
      "   Batch    40 of   638. Elapsed: 0:00:21.\n",
      "   Current average loss = 0.09761525487992913\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.10371322167726854\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.1012673792312853\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.09772414110600948\n",
      "   Batch   120 of   638. Elapsed: 0:01:02.\n",
      "   Current average loss = 0.09433978108378747\n",
      "   Batch   140 of   638. Elapsed: 0:01:12.\n",
      "   Current average loss = 0.09319133095975433\n",
      "   Batch   160 of   638. Elapsed: 0:01:22.\n",
      "   Current average loss = 0.09928811850259081\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.09860458092557059\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.09974614743608981\n",
      "   Batch   220 of   638. Elapsed: 0:01:53.\n",
      "   Current average loss = 0.09847581325183538\n",
      "   Batch   240 of   638. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.09692222635882596\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.09925160193815827\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.0988911101128906\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.09779529609406988\n",
      "   Batch   320 of   638. Elapsed: 0:02:44.\n",
      "   Current average loss = 0.09764683667453937\n",
      "   Batch   340 of   638. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.09772375527878895\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.09754755352106359\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.09646477417432164\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.09608426455175505\n",
      "   Batch   420 of   638. Elapsed: 0:03:35.\n",
      "   Current average loss = 0.09710975713983533\n",
      "   Batch   440 of   638. Elapsed: 0:03:45.\n",
      "   Current average loss = 0.09741148361089555\n",
      "   Batch   460 of   638. Elapsed: 0:03:55.\n",
      "   Current average loss = 0.09746786432907634\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.09706497860218709\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.09826886017620563\n",
      "   Batch   520 of   638. Elapsed: 0:04:26.\n",
      "   Current average loss = 0.09987224811831345\n",
      "   Batch   540 of   638. Elapsed: 0:04:36.\n",
      "   Current average loss = 0.09860586073784226\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.09831282126501069\n",
      "   Batch   580 of   638. Elapsed: 0:04:56.\n",
      "   Current average loss = 0.09801838589944588\n",
      "   Batch   600 of   638. Elapsed: 0:05:06.\n",
      "   Current average loss = 0.09843184014394259\n",
      "   Batch   620 of   638. Elapsed: 0:05:16.\n",
      "   Current average loss = 0.09818775345976916\n",
      "Average training loss = 0.09708403374453814\n",
      "Training epoch took = 0:05:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.2579255309939699\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.08792280834168195\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.07658830345608295\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.06910958352188269\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.067804175146739\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.06696173426462337\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.06867153316852637\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.07699161366326734\n",
      "   Batch   160 of   638. Elapsed: 0:01:21.\n",
      "   Current average loss = 0.07582724028761731\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.0749460217400661\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.07246515075559729\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.07202837311726233\n",
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.07169294314614187\n",
      "   Batch   260 of   638. Elapsed: 0:02:12.\n",
      "   Current average loss = 0.07025713805252543\n",
      "   Batch   280 of   638. Elapsed: 0:02:22.\n",
      "   Current average loss = 0.07130441241232412\n",
      "   Batch   300 of   638. Elapsed: 0:02:32.\n",
      "   Current average loss = 0.07155842236243189\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.07216832201520447\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.07243270538062514\n",
      "   Batch   360 of   638. Elapsed: 0:03:03.\n",
      "   Current average loss = 0.07290883441326312\n",
      "   Batch   380 of   638. Elapsed: 0:03:13.\n",
      "   Current average loss = 0.07417364260549413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Batch   400 of   638. Elapsed: 0:03:23.\n",
      "   Current average loss = 0.07422086712787859\n",
      "   Batch   420 of   638. Elapsed: 0:03:33.\n",
      "   Current average loss = 0.07314619978424161\n",
      "   Batch   440 of   638. Elapsed: 0:03:44.\n",
      "   Current average loss = 0.07403147812318904\n",
      "   Batch   460 of   638. Elapsed: 0:03:54.\n",
      "   Current average loss = 0.07392048978793395\n",
      "   Batch   480 of   638. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.07456916815814717\n",
      "   Batch   500 of   638. Elapsed: 0:04:14.\n",
      "   Current average loss = 0.07513615748193116\n",
      "   Batch   520 of   638. Elapsed: 0:04:24.\n",
      "   Current average loss = 0.07470798383807191\n",
      "   Batch   540 of   638. Elapsed: 0:04:34.\n",
      "   Current average loss = 0.07464414634138208\n",
      "   Batch   560 of   638. Elapsed: 0:04:44.\n",
      "   Current average loss = 0.07400515451611551\n",
      "   Batch   580 of   638. Elapsed: 0:04:55.\n",
      "   Current average loss = 0.07362427026483005\n",
      "   Batch   600 of   638. Elapsed: 0:05:05.\n",
      "   Current average loss = 0.0735316483831654\n",
      "   Batch   620 of   638. Elapsed: 0:05:15.\n",
      "   Current average loss = 0.07311895065039636\n",
      "Average training loss = 0.07257990319746017\n",
      "Training epoch took = 0:05:24\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.415332542076497\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.1017264329828322\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.08638733398984186\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.07273671972798183\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.06954089887149166\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.06593745087506249\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.06377222726393181\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.06303427186295656\n",
      "   Batch   160 of   638. Elapsed: 0:01:22.\n",
      "   Current average loss = 0.05995878473768244\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.059514437135981604\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.06027958411839791\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.059152719582727345\n",
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.05843343276549907\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.05892887768568471\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.05758097787038423\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.057890910344043124\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.057984269689040954\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.057507594290655106\n",
      "   Batch   360 of   638. Elapsed: 0:03:03.\n",
      "   Current average loss = 0.05753871755506326\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.05756912138251784\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.058421396777848716\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.05806366856864077\n",
      "   Batch   440 of   638. Elapsed: 0:03:44.\n",
      "   Current average loss = 0.05784792618519118\n",
      "   Batch   460 of   638. Elapsed: 0:03:54.\n",
      "   Current average loss = 0.0582551889693486\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.05732143442837696\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.05733514587872196\n",
      "   Batch   520 of   638. Elapsed: 0:04:25.\n",
      "   Current average loss = 0.05701366586783059\n",
      "   Batch   540 of   638. Elapsed: 0:04:35.\n",
      "   Current average loss = 0.05731314136916168\n",
      "   Batch   560 of   638. Elapsed: 0:04:45.\n",
      "   Current average loss = 0.0569679682680414\n",
      "   Batch   580 of   638. Elapsed: 0:04:55.\n",
      "   Current average loss = 0.056885586500043404\n",
      "   Batch   600 of   638. Elapsed: 0:05:05.\n",
      "   Current average loss = 0.05657278492746021\n",
      "   Batch   620 of   638. Elapsed: 0:05:16.\n",
      "   Current average loss = 0.05652759099508741\n",
      "Average training loss = 0.056480230788461615\n",
      "Training epoch took = 0:05:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.25894707013708607\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.04644373601768166\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.051562376215588304\n",
      "   Batch    60 of   638. Elapsed: 0:00:30.\n",
      "   Current average loss = 0.050498420597674946\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.04828374292119406\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.047914921515621246\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.04619287960619355\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.04689447805618069\n",
      "   Batch   160 of   638. Elapsed: 0:01:21.\n",
      "   Current average loss = 0.04692700816376601\n",
      "   Batch   180 of   638. Elapsed: 0:01:31.\n",
      "   Current average loss = 0.046807512715976274\n",
      "   Batch   200 of   638. Elapsed: 0:01:41.\n",
      "   Current average loss = 0.04587518879328854\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.044575755125631326\n",
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.04452012064575683\n",
      "   Batch   260 of   638. Elapsed: 0:02:12.\n",
      "   Current average loss = 0.04474739482214388\n",
      "   Batch   280 of   638. Elapsed: 0:02:22.\n",
      "   Current average loss = 0.04473857168590517\n",
      "   Batch   300 of   638. Elapsed: 0:02:32.\n",
      "   Current average loss = 0.04459712808098023\n",
      "   Batch   320 of   638. Elapsed: 0:02:42.\n",
      "   Current average loss = 0.043491828851256284\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.043073377037859135\n",
      "   Batch   360 of   638. Elapsed: 0:03:03.\n",
      "   Current average loss = 0.04286486818972561\n",
      "   Batch   380 of   638. Elapsed: 0:03:13.\n",
      "   Current average loss = 0.042314685819866624\n",
      "   Batch   400 of   638. Elapsed: 0:03:23.\n",
      "   Current average loss = 0.04240338854608126\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.04192186618401181\n",
      "   Batch   440 of   638. Elapsed: 0:03:44.\n",
      "   Current average loss = 0.04167632111983204\n",
      "   Batch   460 of   638. Elapsed: 0:03:54.\n",
      "   Current average loss = 0.042559021748804854\n",
      "   Batch   480 of   638. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.042388506126007995\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.041806043768301604\n",
      "   Batch   520 of   638. Elapsed: 0:04:25.\n",
      "   Current average loss = 0.04191797160847972\n",
      "   Batch   540 of   638. Elapsed: 0:04:35.\n",
      "   Current average loss = 0.0417436889239104\n",
      "   Batch   560 of   638. Elapsed: 0:04:45.\n",
      "   Current average loss = 0.04150368137260167\n",
      "   Batch   580 of   638. Elapsed: 0:04:55.\n",
      "   Current average loss = 0.04118220900980242\n",
      "   Batch   600 of   638. Elapsed: 0:05:06.\n",
      "   Current average loss = 0.04077546281914692\n",
      "   Batch   620 of   638. Elapsed: 0:05:16.\n",
      "   Current average loss = 0.04055218672578884\n",
      "Average training loss = 0.040447093491467315\n",
      "Training epoch took = 0:05:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.27592976672023956\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.04513088089879602\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.04098027179716155\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.03949878071822847\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.036570438536000435\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.037606022742111236\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.03660755076562054\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.03665438869003473\n",
      "   Batch   160 of   638. Elapsed: 0:01:21.\n",
      "   Current average loss = 0.03549439021226135\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.03620685511592051\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.03676174019754399\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.03708357099411924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.03676180217929262\n",
      "   Batch   260 of   638. Elapsed: 0:02:13.\n",
      "   Current average loss = 0.0367840432038065\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.036729433295632975\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.036359845565554376\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.03621090354681655\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.035956274449263755\n",
      "   Batch   360 of   638. Elapsed: 0:03:04.\n",
      "   Current average loss = 0.035781770514828774\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.03528091569912105\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.03509451700272621\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.03510170080269399\n",
      "   Batch   440 of   638. Elapsed: 0:03:44.\n",
      "   Current average loss = 0.03484723217565757\n",
      "   Batch   460 of   638. Elapsed: 0:03:55.\n",
      "   Current average loss = 0.034919234120373045\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.0346528518441725\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.034387498860363845\n",
      "   Batch   520 of   638. Elapsed: 0:04:25.\n",
      "   Current average loss = 0.03400338600185933\n",
      "   Batch   540 of   638. Elapsed: 0:04:35.\n",
      "   Current average loss = 0.03398286041952411\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.03406000654971909\n",
      "   Batch   580 of   638. Elapsed: 0:04:56.\n",
      "   Current average loss = 0.03373553281467295\n",
      "   Batch   600 of   638. Elapsed: 0:05:06.\n",
      "   Current average loss = 0.03372907119424781\n",
      "   Batch   620 of   638. Elapsed: 0:05:16.\n",
      "   Current average loss = 0.033481606766825635\n",
      "Average training loss = 0.03332689871690762\n",
      "Training epoch took = 0:05:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.24199586408868642\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   638. Elapsed: 0:00:10.\n",
      "   Current average loss = 0.026767765777185558\n",
      "   Batch    40 of   638. Elapsed: 0:00:20.\n",
      "   Current average loss = 0.02729516588151455\n",
      "   Batch    60 of   638. Elapsed: 0:00:31.\n",
      "   Current average loss = 0.03121830279317995\n",
      "   Batch    80 of   638. Elapsed: 0:00:41.\n",
      "   Current average loss = 0.031921723997220396\n",
      "   Batch   100 of   638. Elapsed: 0:00:51.\n",
      "   Current average loss = 0.03110919115599245\n",
      "   Batch   120 of   638. Elapsed: 0:01:01.\n",
      "   Current average loss = 0.02966441356790407\n",
      "   Batch   140 of   638. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.029750827433807508\n",
      "   Batch   160 of   638. Elapsed: 0:01:21.\n",
      "   Current average loss = 0.029680513846687972\n",
      "   Batch   180 of   638. Elapsed: 0:01:32.\n",
      "   Current average loss = 0.029164855632310114\n",
      "   Batch   200 of   638. Elapsed: 0:01:42.\n",
      "   Current average loss = 0.029083666952792554\n",
      "   Batch   220 of   638. Elapsed: 0:01:52.\n",
      "   Current average loss = 0.02875382102052258\n",
      "   Batch   240 of   638. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.029573168659893175\n",
      "   Batch   260 of   638. Elapsed: 0:02:12.\n",
      "   Current average loss = 0.02995154345049881\n",
      "   Batch   280 of   638. Elapsed: 0:02:23.\n",
      "   Current average loss = 0.02957695961647135\n",
      "   Batch   300 of   638. Elapsed: 0:02:33.\n",
      "   Current average loss = 0.030137631020043045\n",
      "   Batch   320 of   638. Elapsed: 0:02:43.\n",
      "   Current average loss = 0.02984092338847404\n",
      "   Batch   340 of   638. Elapsed: 0:02:53.\n",
      "   Current average loss = 0.02953654692391865\n",
      "   Batch   360 of   638. Elapsed: 0:03:03.\n",
      "   Current average loss = 0.029436630812577077\n",
      "   Batch   380 of   638. Elapsed: 0:03:14.\n",
      "   Current average loss = 0.029218514769730208\n",
      "   Batch   400 of   638. Elapsed: 0:03:24.\n",
      "   Current average loss = 0.029110196271212772\n",
      "   Batch   420 of   638. Elapsed: 0:03:34.\n",
      "   Current average loss = 0.02907766510865518\n",
      "   Batch   440 of   638. Elapsed: 0:03:44.\n",
      "   Current average loss = 0.028996207374042238\n",
      "   Batch   460 of   638. Elapsed: 0:03:54.\n",
      "   Current average loss = 0.02891156620943271\n",
      "   Batch   480 of   638. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.02873269758565584\n",
      "   Batch   500 of   638. Elapsed: 0:04:15.\n",
      "   Current average loss = 0.028635625022929163\n",
      "   Batch   520 of   638. Elapsed: 0:04:25.\n",
      "   Current average loss = 0.028734007080381093\n",
      "   Batch   540 of   638. Elapsed: 0:04:36.\n",
      "   Current average loss = 0.02856193462028858\n",
      "   Batch   560 of   638. Elapsed: 0:04:46.\n",
      "   Current average loss = 0.02842936354177904\n",
      "   Batch   580 of   638. Elapsed: 0:04:56.\n",
      "   Current average loss = 0.0286901529904471\n",
      "   Batch   600 of   638. Elapsed: 0:05:06.\n",
      "   Current average loss = 0.028506218350764053\n",
      "   Batch   620 of   638. Elapsed: 0:05:16.\n",
      "   Current average loss = 0.028397702627567453\n",
      "Average training loss = 0.02851224621392197\n",
      "Training epoch took = 0:05:25\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.2532781930771512\n",
      "Validation took: 0:00:10\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps) \n",
    "\n",
    "model.zero_grad() \n",
    "\n",
    "best_val_loss = 1e9 \n",
    "\n",
    "for epoch_i in range(0,epochs): \n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs)) \n",
    "    print(\"Training ...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time()-t0)\n",
    "            print('   Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) \n",
    "            print('   Current average loss = {}'.format(total_loss / step)) \n",
    "            \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids = None, \n",
    "                        attention_mask = b_input_masks,\n",
    "                        labels = b_target) \n",
    "        \n",
    "        # using MSE loss \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    \n",
    "    print(\"Average training loss = {}\".format(avg_train_loss)) \n",
    "    print(\"Training epoch took = {}\".format(format_time(time.time() - t0)))\n",
    " \n",
    "    ##### validation ##### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\") \n",
    "    \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    nb_eval_steps, nb_eval_examples = 0,0 \n",
    "    \n",
    "    for batch in validation_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids = None, \n",
    "                            attention_mask = b_input_masks, \n",
    "                            labels = b_target) \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        eval_loss += loss.item() \n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)\n",
    "    print(\"Average validation loss = {}\".format(avg_val_loss)) \n",
    "    print(\"Validation took: {:}\".format(format_time(time.time()-t0))) \n",
    "    \n",
    "    if avg_val_loss < best_val_loss: \n",
    "        best_val_loss = avg_val_loss \n",
    "        torch.save(model.state_dict(), \"RoBERTa_large_\" + str(epoch_i+1)) \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24199586408868642"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load best model \n",
    "test_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1) \n",
    "checkpoint = torch.load('RoBERTa_baseline_6') \n",
    "test_model.load_state_dict(checkpoint)\n",
    "test_model.cuda() \n",
    "\n",
    "test_model.eval() # convert to evaluation mode. \n",
    "print() # avoid printing model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 55.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_texts = test['excerpt'].values \n",
    "\n",
    "predictions = [] \n",
    "\n",
    "for text in tqdm(test_texts, position=0,leave=True): \n",
    "    input_id, attention_mask = roberta_tokenizer(text, MAX_LEN=MAX_LEN) \n",
    "    input_id = torch.tensor(input_id, dtype=int) \n",
    "    attention_mask = torch.tensor(attention_mask, dtype=int) \n",
    "    \n",
    "    input_id = torch.reshape(input_id, (-1,MAX_LEN)) \n",
    "    attention_mask = torch.reshape(attention_mask, (-1,MAX_LEN)) \n",
    "    \n",
    "    input_id = input_id.to(device) \n",
    "    attention_mask = attention_mask.to(device) \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "            outputs = test_model(input_id, \n",
    "                                 token_type_ids=None, \n",
    "                                 attention_mask=attention_mask) \n",
    "    \n",
    "    yhat = outputs[0].item() \n",
    "    predictions.append(yhat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>0.372574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.225584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.119132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.694268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.715674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-0.712704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.168142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661  0.372574\n",
       "1  f0953f0a5 -0.225584\n",
       "2  0df072751 -0.119132\n",
       "3  04caf4e0c -2.694268\n",
       "4  0e63f8bea -1.715674\n",
       "5  12537fe78 -0.712704\n",
       "6  965e592c0  0.168142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[:,1] = predictions \n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
