{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0+cu110 available.\n",
      "TensorFlow version 2.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import * \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "import time \n",
    "import datetime \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2834, 6), (7, 4), (7, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv') \n",
    "test = pd.read_csv('./data/test.csv') \n",
    "submission = pd.read_csv('./data/sample_submission.csv') \n",
    "\n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'] = train['excerpt'].apply(lambda x : len(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2834.000000\n",
       "mean      971.732886\n",
       "std       117.257578\n",
       "min       669.000000\n",
       "25%       885.000000\n",
       "50%       971.000000\n",
       "75%      1058.000000\n",
       "max      1341.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train['excerpt'].values \n",
    "train_targets = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-large-discriminator\") \n",
    "\n",
    "def electra_tokenizer(sent, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    if len(input_id) > 512: \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_maks[:129] + attention_mask[-383:] \n",
    "        print(\"Long Text!! Using Head+Tail Truncation\") \n",
    "    elif len(input_id) <= 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512-len(attention_mask)) \n",
    "        \n",
    "    return input_id, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10 \n",
    "VALID_SPLIT = 0.1 \n",
    "MAX_LEN = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:14<00:00, 201.30it/s]\n"
     ]
    }
   ],
   "source": [
    "N = train_texts.shape[0] \n",
    "\n",
    "input_ids = np.zeros((N,MAX_LEN),dtype=int) \n",
    "attention_masks = np.zeros((N, MAX_LEN),dtype=int) \n",
    "targets = np.zeros((N), dtype=np.double) \n",
    "\n",
    "for i in tqdm(range(N), position = 0, leave=True): \n",
    "    try: \n",
    "        cur_str = train_texts[i] \n",
    "        cur_target = train_targets[i] \n",
    "        input_id, attention_mask = electra_tokenizer(cur_str, MAX_LEN=MAX_LEN) \n",
    "        input_ids[i,] = input_id \n",
    "        attention_masks[i,] = attention_mask \n",
    "        targets[i,] = cur_target \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(cur_str) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2550, 512]), torch.Size([2550, 512]), torch.Size([2550]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int) \n",
    "targets = torch.tensor(targets, dtype=torch.float32) \n",
    "\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_ids, targets, random_state = 84, test_size=VALID_SPLIT)\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(attention_masks, targets, random_state = 84, test_size=VALID_SPLIT) \n",
    "\n",
    "train_inputs.shape, train_attention_mask.shape, train_targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([284, 512]), torch.Size([284, 512]), torch.Size([284]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_attention_mask.shape, val_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_attention_mask, train_targets) \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE) \n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_attention_mask, val_targets)\n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"google/electra-large-discriminator\", num_labels=1) \n",
    "model.cuda() \n",
    "print() # avoid printing model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):  \n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:18.\n",
      "   Current average loss = 0.963998019695282\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.8611662644892931\n",
      "   Batch    60 of   319. Elapsed: 0:00:52.\n",
      "   Current average loss = 0.774686798453331\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.7445972943678498\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.6895752669870854\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.6539979668334126\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.6126591837831906\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.603658590791747\n",
      "   Batch   180 of   319. Elapsed: 0:02:38.\n",
      "   Current average loss = 0.5974167899125152\n",
      "   Batch   200 of   319. Elapsed: 0:02:55.\n",
      "   Current average loss = 0.5732560712657869\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.5664583317935467\n",
      "   Batch   240 of   319. Elapsed: 0:03:30.\n",
      "   Current average loss = 0.5572802243133386\n",
      "   Batch   260 of   319. Elapsed: 0:03:48.\n",
      "   Current average loss = 0.5384606365687572\n",
      "   Batch   280 of   319. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.533113003069801\n",
      "   Batch   300 of   319. Elapsed: 0:04:23.\n",
      "   Current average loss = 0.5233134374767542\n",
      "Average training loss = 0.5202512204833912\n",
      "Training epoch took = 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.5258849782662259\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:18.\n",
      "   Current average loss = 0.384818896278739\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.3407638868317008\n",
      "   Batch    60 of   319. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.3251199525470535\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.29570437748916445\n",
      "   Batch   100 of   319. Elapsed: 0:01:28.\n",
      "   Current average loss = 0.2829897118732333\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.2755377417119841\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.26606616063841754\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.2719319587107748\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.27356348952485454\n",
      "   Batch   200 of   319. Elapsed: 0:02:55.\n",
      "   Current average loss = 0.2726250016503036\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.2725906710902398\n",
      "   Batch   240 of   319. Elapsed: 0:03:30.\n",
      "   Current average loss = 0.26869499548338355\n",
      "   Batch   260 of   319. Elapsed: 0:03:48.\n",
      "   Current average loss = 0.2689906636826121\n",
      "   Batch   280 of   319. Elapsed: 0:04:06.\n",
      "   Current average loss = 0.2671914683255766\n",
      "   Batch   300 of   319. Elapsed: 0:04:23.\n",
      "   Current average loss = 0.26231921368589006\n",
      "Average training loss = 0.2574195176199499\n",
      "Training epoch took = 0:04:40\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3027442776494556\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 3 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:18.\n",
      "   Current average loss = 0.1320124814286828\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.15943646179512144\n",
      "   Batch    60 of   319. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.15603851809476812\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.14913177315611392\n",
      "   Batch   100 of   319. Elapsed: 0:01:28.\n",
      "   Current average loss = 0.14472795944660902\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.14073428884148598\n",
      "   Batch   140 of   319. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.13918209139789853\n",
      "   Batch   160 of   319. Elapsed: 0:02:21.\n",
      "   Current average loss = 0.13561964572872967\n",
      "   Batch   180 of   319. Elapsed: 0:02:38.\n",
      "   Current average loss = 0.14257951138748062\n",
      "   Batch   200 of   319. Elapsed: 0:02:56.\n",
      "   Current average loss = 0.13860768113285304\n",
      "   Batch   220 of   319. Elapsed: 0:03:13.\n",
      "   Current average loss = 0.13771720592948525\n",
      "   Batch   240 of   319. Elapsed: 0:03:31.\n",
      "   Current average loss = 0.13795733912847935\n",
      "   Batch   260 of   319. Elapsed: 0:03:48.\n",
      "   Current average loss = 0.1411335724620865\n",
      "   Batch   280 of   319. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.14471699825621076\n",
      "   Batch   300 of   319. Elapsed: 0:04:23.\n",
      "   Current average loss = 0.14365244244535763\n",
      "Average training loss = 0.1406687242460662\n",
      "Training epoch took = 0:04:40\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.45161813125014305\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 4 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:17.\n",
      "   Current average loss = 0.09967767698690295\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.09877156321890652\n",
      "   Batch    60 of   319. Elapsed: 0:00:52.\n",
      "   Current average loss = 0.08774519643435875\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.08929595109075308\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.08506771689280868\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.08673584416198234\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.08749860879033804\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.08569779128301888\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.08534364687899748\n",
      "   Batch   200 of   319. Elapsed: 0:02:55.\n",
      "   Current average loss = 0.08436247038654983\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.08318033624779095\n",
      "   Batch   240 of   319. Elapsed: 0:03:30.\n",
      "   Current average loss = 0.08198032881288479\n",
      "   Batch   260 of   319. Elapsed: 0:03:47.\n",
      "   Current average loss = 0.08238659650087357\n",
      "   Batch   280 of   319. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.08212614659486073\n",
      "   Batch   300 of   319. Elapsed: 0:04:22.\n",
      "   Current average loss = 0.08255261567731699\n",
      "Average training loss = 0.08267028418407545\n",
      "Training epoch took = 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3322559321920077\n",
      "Validation took: 0:00:09\n",
      "\n",
      "======= Epoch 5 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:17.\n",
      "   Current average loss = 0.11508490964770317\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.09535657595843076\n",
      "   Batch    60 of   319. Elapsed: 0:00:52.\n",
      "   Current average loss = 0.08173657652611534\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.08207301304209977\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.0879094803892076\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.10439594889370103\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.1539499665078308\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.15206848275847734\n",
      "   Batch   180 of   319. Elapsed: 0:02:38.\n",
      "   Current average loss = 0.1492504108697176\n",
      "   Batch   200 of   319. Elapsed: 0:02:56.\n",
      "   Current average loss = 0.1519683755747974\n",
      "   Batch   220 of   319. Elapsed: 0:03:13.\n",
      "   Current average loss = 0.14429034415463154\n",
      "   Batch   240 of   319. Elapsed: 0:03:30.\n",
      "   Current average loss = 0.13551042548691233\n",
      "   Batch   260 of   319. Elapsed: 0:03:48.\n",
      "   Current average loss = 0.12987815878855494\n",
      "   Batch   280 of   319. Elapsed: 0:04:05.\n",
      "   Current average loss = 0.12420396537386945\n",
      "   Batch   300 of   319. Elapsed: 0:04:23.\n",
      "   Current average loss = 0.11852440233963231\n",
      "Average training loss = 0.1140010334860802\n",
      "Training epoch took = 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3100993860926893\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 6 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:17.\n",
      "   Current average loss = 0.035766927525401115\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.04133854107931256\n",
      "   Batch    60 of   319. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.040218251400316755\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.04394034494180232\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.042402669675648215\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.041288927554463346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.04029890782465892\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.0403666461177636\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.04124160390864644\n",
      "   Batch   200 of   319. Elapsed: 0:02:55.\n",
      "   Current average loss = 0.041473537236452106\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.041926716458560394\n",
      "   Batch   240 of   319. Elapsed: 0:03:29.\n",
      "   Current average loss = 0.04161102710058913\n",
      "   Batch   260 of   319. Elapsed: 0:03:47.\n",
      "   Current average loss = 0.04303386415211627\n",
      "   Batch   280 of   319. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.04310386467259377\n",
      "   Batch   300 of   319. Elapsed: 0:04:22.\n",
      "   Current average loss = 0.04314722948397199\n",
      "Average training loss = 0.043240444870449625\n",
      "Training epoch took = 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.27644926961511374\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 7 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:17.\n",
      "   Current average loss = 0.032809321233071384\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.03293714107712731\n",
      "   Batch    60 of   319. Elapsed: 0:00:52.\n",
      "   Current average loss = 0.034520122090665004\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.03332268961821683\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.03421322845388204\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.03520123400182153\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.03666706138756126\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.03906049548822921\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.03821807077102777\n",
      "   Batch   200 of   319. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.037719650787767026\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.03721909877971153\n",
      "   Batch   240 of   319. Elapsed: 0:03:29.\n",
      "   Current average loss = 0.03641977977434484\n",
      "   Batch   260 of   319. Elapsed: 0:03:47.\n",
      "   Current average loss = 0.03549125523915371\n",
      "   Batch   280 of   319. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.03486497201125271\n",
      "   Batch   300 of   319. Elapsed: 0:04:21.\n",
      "   Current average loss = 0.03476552878351261\n",
      "Average training loss = 0.03504096496949515\n",
      "Training epoch took = 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.32940790615975857\n",
      "Validation took: 0:00:09\n",
      "\n",
      "======= Epoch 8 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:18.\n",
      "   Current average loss = 0.02911541434004903\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.0273343563079834\n",
      "   Batch    60 of   319. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.02464085917454213\n",
      "   Batch    80 of   319. Elapsed: 0:01:11.\n",
      "   Current average loss = 0.023966223961906508\n",
      "   Batch   100 of   319. Elapsed: 0:01:28.\n",
      "   Current average loss = 0.02328164520673454\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.02282100235267232\n",
      "   Batch   140 of   319. Elapsed: 0:02:03.\n",
      "   Current average loss = 0.02251029122354729\n",
      "   Batch   160 of   319. Elapsed: 0:02:20.\n",
      "   Current average loss = 0.021792852194630542\n",
      "   Batch   180 of   319. Elapsed: 0:02:38.\n",
      "   Current average loss = 0.023856186335130283\n",
      "   Batch   200 of   319. Elapsed: 0:02:55.\n",
      "   Current average loss = 0.022902381710009648\n",
      "   Batch   220 of   319. Elapsed: 0:03:13.\n",
      "   Current average loss = 0.023244024111508308\n",
      "   Batch   240 of   319. Elapsed: 0:03:31.\n",
      "   Current average loss = 0.02251384383707773\n",
      "   Batch   260 of   319. Elapsed: 0:03:49.\n",
      "   Current average loss = 0.022900555677747784\n",
      "   Batch   280 of   319. Elapsed: 0:04:07.\n",
      "   Current average loss = 0.022634557234622273\n",
      "   Batch   300 of   319. Elapsed: 0:04:24.\n",
      "   Current average loss = 0.022566898548199484\n",
      "Average training loss = 0.022507729156958787\n",
      "Training epoch took = 0:04:40\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.35447261668741703\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 9 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:18.\n",
      "   Current average loss = 0.016594638070091604\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.018096184614114462\n",
      "   Batch    60 of   319. Elapsed: 0:00:53.\n",
      "   Current average loss = 0.017768475972115994\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.0174856289930176\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.017937803007662297\n",
      "   Batch   120 of   319. Elapsed: 0:01:44.\n",
      "   Current average loss = 0.018280581588624046\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.01783054197107309\n",
      "   Batch   160 of   319. Elapsed: 0:02:19.\n",
      "   Current average loss = 0.017588315730972682\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.017372796507293564\n",
      "   Batch   200 of   319. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.01716917014098726\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.017116053725211796\n",
      "   Batch   240 of   319. Elapsed: 0:03:29.\n",
      "   Current average loss = 0.017368661233922465\n",
      "   Batch   260 of   319. Elapsed: 0:03:46.\n",
      "   Current average loss = 0.017417942109302833\n",
      "   Batch   280 of   319. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.017109426014524486\n",
      "   Batch   300 of   319. Elapsed: 0:04:21.\n",
      "   Current average loss = 0.017031076459679752\n",
      "Average training loss = 0.01688662625128129\n",
      "Training epoch took = 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3322069286886189\n",
      "Validation took: 0:00:10\n",
      "\n",
      "======= Epoch 10 / 10 =======\n",
      "Training ...\n",
      "   Batch    20 of   319. Elapsed: 0:00:17.\n",
      "   Current average loss = 0.011139376787468791\n",
      "   Batch    40 of   319. Elapsed: 0:00:35.\n",
      "   Current average loss = 0.011298219102900475\n",
      "   Batch    60 of   319. Elapsed: 0:00:52.\n",
      "   Current average loss = 0.011155380099080504\n",
      "   Batch    80 of   319. Elapsed: 0:01:10.\n",
      "   Current average loss = 0.011372369877062739\n",
      "   Batch   100 of   319. Elapsed: 0:01:27.\n",
      "   Current average loss = 0.011500886310823261\n",
      "   Batch   120 of   319. Elapsed: 0:01:45.\n",
      "   Current average loss = 0.0117077688628342\n",
      "   Batch   140 of   319. Elapsed: 0:02:02.\n",
      "   Current average loss = 0.011334065841843507\n",
      "   Batch   160 of   319. Elapsed: 0:02:19.\n",
      "   Current average loss = 0.011436846386641265\n",
      "   Batch   180 of   319. Elapsed: 0:02:37.\n",
      "   Current average loss = 0.011415024804106604\n",
      "   Batch   200 of   319. Elapsed: 0:02:54.\n",
      "   Current average loss = 0.011214430738473311\n",
      "   Batch   220 of   319. Elapsed: 0:03:12.\n",
      "   Current average loss = 0.011308808864983307\n",
      "   Batch   240 of   319. Elapsed: 0:03:29.\n",
      "   Current average loss = 0.011242892251660426\n",
      "   Batch   260 of   319. Elapsed: 0:03:47.\n",
      "   Current average loss = 0.011254974801200801\n",
      "   Batch   280 of   319. Elapsed: 0:04:04.\n",
      "   Current average loss = 0.011128320698792648\n",
      "   Batch   300 of   319. Elapsed: 0:04:22.\n",
      "   Current average loss = 0.011173104779639592\n",
      "Average training loss = 0.01135345296656894\n",
      "Training epoch took = 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "Average validation loss = 0.3379885986861255\n",
      "Validation took: 0:00:10\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps) \n",
    "\n",
    "model.zero_grad() \n",
    "\n",
    "best_val_loss = 1e9 \n",
    "\n",
    "for epoch_i in range(0,epochs): \n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs)) \n",
    "    print(\"Training ...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time()-t0)\n",
    "            print('   Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) \n",
    "            print('   Current average loss = {}'.format(total_loss / step)) \n",
    "            \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids = None, \n",
    "                        attention_mask = b_input_masks,\n",
    "                        labels = b_target) \n",
    "        \n",
    "        # using MSE loss \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    \n",
    "    print(\"Average training loss = {}\".format(avg_train_loss)) \n",
    "    print(\"Training epoch took = {}\".format(format_time(time.time() - t0)))\n",
    " \n",
    "    ##### validation ##### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\") \n",
    "    \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    nb_eval_steps, nb_eval_examples = 0,0 \n",
    "    \n",
    "    for batch in validation_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_input_ids, b_input_masks, b_target = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids = None, \n",
    "                            attention_mask = b_input_masks, \n",
    "                            labels = b_target) \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        eval_loss += loss.item() \n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)\n",
    "    print(\"Average validation loss = {}\".format(avg_val_loss)) \n",
    "    print(\"Validation took: {:}\".format(format_time(time.time()-t0))) \n",
    "    \n",
    "    if avg_val_loss < best_val_loss: \n",
    "        best_val_loss = avg_val_loss \n",
    "        torch.save(model.state_dict(), \"ELECTRA_large_\" + str(epoch_i+1)) \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27644926961511374"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
